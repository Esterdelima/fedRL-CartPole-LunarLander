{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mario.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NES Environment"
      ],
      "metadata": {
        "id": "U5Zpi_aKcVYR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tQ0ChH2FI1mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769ecf0b-1f0c-4698-f411-aa1d9020e100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.3.0 in /usr/local/lib/python3.7/dist-packages (7.3.0)\n",
            "Requirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros==7.3.0) (8.1.8)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.21.5)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym-super-mario-bros==7.3.0\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QU1-XZL_cewu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "4EBzBrX_GNWD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_test = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "print(env_test.observation_space.shape)\n",
        "print(env_test.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9onIgm6dImw",
        "outputId": "8c6a5874-17d6-4ebd-b512-d7c2f1c2b0ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3)\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info"
      ],
      "metadata": {
        "id": "9Q_m_d2DCRyG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, \n",
        "                                         shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation"
      ],
      "metadata": {
        "id": "eu_BDqyBCfvE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, \n",
        "                                         shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation"
      ],
      "metadata": {
        "id": "pkXKwY5Adbuz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mario_env(env_name):\n",
        "    env = gym_super_mario_bros.make(env_name)\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = GrayScaleObservation(env)\n",
        "    env = ResizeObservation(env, shape=84)\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "    return JoypadSpace(env, SIMPLE_MOVEMENT)"
      ],
      "metadata": {
        "id": "BMdORGcbYcmt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, state_shape, action_space, batch_size=32, max_size=10000,\n",
        "                 load=False, path=None):\n",
        "        self.path = path + 'buffer/'\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        if load:\n",
        "            self.load()\n",
        "        else:\n",
        "            self.next = 0\n",
        "            self.size = 0\n",
        "\n",
        "            self.states = torch.empty((max_size, *state_shape))\n",
        "            self.actions = torch.empty((max_size, 1), dtype=torch.int64)\n",
        "            self.rewards = torch.empty((max_size, 1))\n",
        "            self.states_p = torch.empty((max_size, *state_shape))\n",
        "            self.is_terminals = torch.empty((max_size, 1), dtype=torch.float)\n",
        "\n",
        "\n",
        "    def __len__(self): return self.size\n",
        "    \n",
        "\n",
        "    def store(self, state, action, reward, state_p, is_terminal):\n",
        "        state = state.__array__()\n",
        "        state_p = state_p.__array__()\n",
        "\n",
        "        self.states[self.next] = torch.tensor(state)\n",
        "        self.actions[self.next] = action\n",
        "        self.rewards[self.next] = reward\n",
        "        self.states_p[self.next] = torch.tensor(state_p)\n",
        "        self.is_terminals[self.next] = is_terminal\n",
        "\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.next = (self.next + 1) % self.max_size\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        indices = np.random.choice(self.size, size=self.batch_size, \n",
        "                                   replace=False)\n",
        "        return self.states[indices], \\\n",
        "            self.actions[indices], \\\n",
        "            self.rewards[indices], \\\n",
        "            self.states_p[indices], \\\n",
        "            self.is_terminals[indices]\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        self.next = 0\n",
        "        self.size = 0\n",
        "        self.states = torch.empty_like(self.states)\n",
        "        self.actions = torch.empty_like(self.actions)\n",
        "        self.rewards = torch.empty_like(self.rewards)\n",
        "        self.states_p = torch.empty_like(self.states_p)\n",
        "        self.is_terminals = torch.empty_like(self.is_terminals)\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        with open(self.path + \"next.pkl\", 'rb') as f:\n",
        "            self.next = pickle.load(f)\n",
        "        with open(self.path + \"size.pkl\", 'rb') as f:\n",
        "            self.size = pickle.load(f)\n",
        "        self.states = torch.load(self.path + \"states.pt\")\n",
        "        self.actions = torch.load(self.path + \"actions.pt\")\n",
        "        self.rewards = torch.load(self.path + \"rewards.pt\")\n",
        "        self.states_p = torch.load(self.path + \"states_p.pt\")\n",
        "        self.is_terminals = torch.load(self.path + \"is_terminals.pt\")\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
        "        with open(self.path + \"next.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.next, f)\n",
        "        with open(self.path + \"size.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.size, f)\n",
        "        torch.save(self.states, self.path + \"states.pt\")\n",
        "        torch.save(self.actions, self.path + \"actions.pt\")\n",
        "        torch.save(self.rewards, self.path + \"rewards.pt\")\n",
        "        torch.save(self.states_p, self.path + \"states_p.pt\")\n",
        "        torch.save(self.is_terminals, self.path + \"is_terminals.pt\")"
      ],
      "metadata": {
        "id": "9S04urIOAL4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_shape, actions_size, \n",
        "                optimizer=torch.optim.Adam, learning_rate=0.00025):\n",
        "        super().__init__()\n",
        "        self.personalized = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.shared = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(3136, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, actions_size)\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "\n",
        "    def format_(self, states):\n",
        "        if not isinstance(states, torch.Tensor):\n",
        "            states = torch.tensor(states, dtype=torch.float32)\n",
        "        return states\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        states = self.format_(x)\n",
        "        out = self.personalized(states)\n",
        "        out = self.shared(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def update_netowrk(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "zifT_IS_0cj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, id, env_name, env_fn, Qnet=QNetwork, buffer=ReplayBuffer,\n",
        "                 max_epsilon=1, min_epsilon=0.05, epsilon_decay=0.99, gamma=0.9,\n",
        "                 target_update_rate=2000, min_buffer=100, \n",
        "                 load=False, path=None) -> None:\n",
        "        self.id = id\n",
        "        self.path = path + str(id) + \"/\"\n",
        "\n",
        "        self.env = env_fn(env_name)\n",
        "        self.env_fn = env_fn\n",
        "        self.n_actions = self.env.action_space.n\n",
        "        self.state_shape = self.env.observation_space.shape\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.min_buffer = min_buffer\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.gamma = gamma\n",
        "        self.target_update_rate = target_update_rate\n",
        "        self.buffer = buffer(self.state_shape, self.n_actions,\n",
        "                             load=load, path=self.path)\n",
        "\n",
        "        self.online_net = Qnet(self.state_shape, self.n_actions).to(self.device)\n",
        "        self.target_net = Qnet(self.state_shape, self.n_actions).to(self.device)\n",
        "\n",
        "        if load:\n",
        "            self.load()\n",
        "        else:\n",
        "            self.update_target_network()\n",
        "            self.epsilon = max_epsilon\n",
        "            self.step_count = 0\n",
        "            self.episode_count = 0\n",
        "            self.rewards = []\n",
        "\n",
        "    \n",
        "    def load(self):\n",
        "        with open(self.path + \"step_count.pkl\", 'rb') as f:\n",
        "            self.step_count = pickle.load(f)\n",
        "        with open(self.path + \"episode_count.pkl\", 'rb') as f:\n",
        "            self.episode_count = pickle.load(f)\n",
        "        with open(self.path + \"rewards.pkl\", 'rb') as f:\n",
        "            self.rewards = pickle.load(f)\n",
        "        with open(self.path + \"epsilon.pkl\", 'rb') as f:\n",
        "            self.epsilon = pickle.load(f)\n",
        "        self.online_net.load_state_dict(torch.load(self.path + \"online_net.pt\", \n",
        "                                                   map_location=torch.device(self.device)))\n",
        "        self.target_net.load_state_dict(torch.load(self.path + \"target_net.pt\", \n",
        "                                                   map_location=torch.device(self.device)))\n",
        "\n",
        "    def save(self):\n",
        "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
        "        self.buffer.save()\n",
        "        with open(self.path + \"step_count.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.step_count, f)\n",
        "        with open(self.path + \"episode_count.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.episode_count, f)\n",
        "        with open(self.path + \"rewards.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.rewards, f)\n",
        "        with open(self.path + \"epsilon.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.epsilon, f)\n",
        "        torch.save(self.online_net.state_dict(), self.path +  \"online_net.pt\")\n",
        "        torch.save(self.target_net.state_dict(), self.path +  \"target_net.pt\")\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, n_episodes):\n",
        "        for i in tqdm(range(n_episodes)):\n",
        "            episode_reward = 0\n",
        "            state = self.env.reset()\n",
        "\n",
        "            while True:\n",
        "                self.step_count += 1\n",
        "                action = self.epsilonGreedyPolicy(state)\n",
        "                state_p, reward, done, info = self.env.step(action)\n",
        "                episode_reward += reward\n",
        "\n",
        "                is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
        "                is_failure = done and not is_truncated\n",
        "                self.buffer.store(state, action, reward, state_p, float(is_failure))\n",
        "\n",
        "                if len(self.buffer) >= self.min_buffer:\n",
        "                    self.update()\n",
        "                    if self.step_count % self.target_update_rate == 0:\n",
        "                        self.update_target_network()\n",
        "\n",
        "                state = state_p\n",
        "                if done:\n",
        "                    self.episode_count += 1\n",
        "                    self.rewards.append(episode_reward)\n",
        "                    break\n",
        "\n",
        "        print(\"Agent-{} Episode {} Step {} score = {}, average score = {}\"\\\n",
        "                .format(self.id, self.episode_count, self.step_count, self.rewards[-1], np.mean(self.rewards)))\n",
        "\n",
        "\n",
        "    def get_score(self):\n",
        "        # return np.mean(self.rewards[-5:])\n",
        "        return 1\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        states, actions, rewards, states_p, is_terminals = self.buffer.sample()\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        states_p = states_p.to(self.device)\n",
        "        is_terminals = is_terminals.to(self.device)\n",
        "\n",
        "        td_estimate = self.online_net(states).gather(1, actions)\n",
        "\n",
        "        actions_p = self.online_net(states).argmax(axis=1, keepdim=True)\n",
        "        with torch.no_grad():\n",
        "            q_states_p = self.target_net(states_p)\n",
        "        q_state_p_action_p = q_states_p.gather(1, actions_p)\n",
        "        td_target = rewards + (1-is_terminals) * self.gamma * q_state_p_action_p\n",
        "\n",
        "        self.online_net.update_netowrk(td_estimate, td_target)\n",
        "        self.update_epsilon()\n",
        "\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
        "\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
        "\n",
        "\n",
        "    def epsilonGreedyPolicy(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = np.random.randint(self.n_actions)\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "            state = torch.tensor(state).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                action = self.online_net(state).argmax().item()\n",
        "        return action"
      ],
      "metadata": {
        "id": "_EBKN7-pBmDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Agent):\n",
        "    def __init__(self, env_names, env_fn, Qnet=QNetwork, load=False, path=None) -> None:\n",
        "        self.path = path + \"global/\"\n",
        "        self.envs = []\n",
        "        for name in env_names:\n",
        "            self.envs.append(env_fn(name))\n",
        "        self.n_actions = self.envs[0].action_space.n\n",
        "        self.state_shape = self.envs[0].observation_space.shape\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.online_net = Qnet(self.state_shape, self.n_actions).to(self.device)\n",
        "        self.target_net = Qnet(self.state_shape, self.n_actions).to(self.device)\n",
        "\n",
        "        if load:\n",
        "            self.load()\n",
        "        else:\n",
        "            self.update_target_network()\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        self.online_net.load_state_dict(torch.load(self.path + \"online_net.pt\", \n",
        "                                                   map_location=torch.device(self.device)))\n",
        "        self.target_net.load_state_dict(torch.load(self.path + \"target_net.pt\", \n",
        "                                                   map_location=torch.device(self.device)))\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
        "        torch.save(self.online_net.state_dict(), self.path + \"online_net.pt\")\n",
        "        torch.save(self.target_net.state_dict(), self.path + \"target_net.pt\")\n",
        "\n",
        "\n",
        "    def get_score(self):\n",
        "        # return np.mean(self.rewards[-5:])\n",
        "        return 1\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        rewards = np.zeros(len(self.envs))\n",
        "        for i in range(len(self.envs)):\n",
        "            r = self.evaluate(i)\n",
        "            rewards[i] = r\n",
        "        return rewards\n",
        "\n",
        "\n",
        "    def evaluate(self, i):\n",
        "        rewards = 0\n",
        "        state = self.envs[i].reset()\n",
        "        while True:\n",
        "            action = self.greedyPolicy(state)\n",
        "            state_p, reward, done, _ = self.envs[i].step(action)\n",
        "            rewards += reward\n",
        "            if done:\n",
        "                break\n",
        "            state = state_p\n",
        "        return rewards\n",
        "\n",
        "\n",
        "    def greedyPolicy(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = state.__array__()\n",
        "            state = torch.tensor(state).unsqueeze(0).to(self.device)\n",
        "            action = self.target_net(state).argmax().item()\n",
        "        return action"
      ],
      "metadata": {
        "id": "weiDR_iqmYkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Federator:\n",
        "    def __init__(self, env_fn, update_rate, path=\"./Mario/\", load=False) -> None:\n",
        "        self.path = path\n",
        "        self.envs = [\n",
        "                'SuperMarioBros-1-1-v0',\n",
        "                'SuperMarioBros-1-2-v0',\n",
        "                'SuperMarioBros-1-3-v0',\n",
        "                'SuperMarioBros-1-4-v0'\n",
        "        ]\n",
        "        self.global_agent = Mario(self.envs, env_fn, load=load, path=self.path)\n",
        "\n",
        "        self.update_rate = update_rate\n",
        "        self.n_agents = 4\n",
        "        self.agents = []\n",
        "        for i in range(self.n_agents):\n",
        "            agent = Agent(i, self.envs[i], env_fn, load=load, path=self.path)\n",
        "            self.agents.append(agent)\n",
        "\n",
        "        if load:\n",
        "            self.load()\n",
        "        else:\n",
        "            self.set_local_networks()\n",
        "            self.rewards = []\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        with open(self.path + \"rewards.pkl\", 'rb') as f:\n",
        "            self.rewards = pickle.load(f)\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
        "        with open(self.path + \"rewards.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.rewards, f)\n",
        "        self.global_agent.save()\n",
        "        for agent in self.agents:\n",
        "            agent.save()\n",
        "        print(\"All Saved to \" + self.path)\n",
        "\n",
        "    def train(self, n_runs):\n",
        "        rewards = np.zeros((n_runs, len(self.envs)))\n",
        "        for i in range(n_runs):\n",
        "            print(\"Iteration: {}\".format(i+1))\n",
        "            scores = []\n",
        "            for agent in self.agents:\n",
        "                agent.train(self.update_rate)\n",
        "                scores.append(agent.get_score())\n",
        "            self.aggregate_networks(scores)\n",
        "            self.set_local_networks()\n",
        "            rewards[i] = self.global_agent.test()\n",
        "            print(rewards[i])\n",
        "        self.save()\n",
        "\n",
        "\n",
        "    def aggregate_networks(self, scores):\n",
        "        sd_online = self.global_agent.online_net.state_dict()\n",
        "        sd_target = self.global_agent.target_net.state_dict()\n",
        "\n",
        "        online_dicts = []\n",
        "        target_dicts = []\n",
        "        for agent in self.agents:\n",
        "            online_dicts.append(agent.online_net.state_dict())\n",
        "            target_dicts.append(agent.target_net.state_dict())\n",
        "\n",
        "        for key in sd_online:\n",
        "            sd_online[key] = torch.zeros_like(sd_online[key])\n",
        "            for i, dict in enumerate(online_dicts):\n",
        "                sd_online[key] += scores[i] * dict[key]\n",
        "            sd_online[key] /= sum(scores)\n",
        "\n",
        "        for key in sd_target:\n",
        "            sd_target[key] = torch.zeros_like(sd_target[key])\n",
        "            for i, dict in enumerate(target_dicts):\n",
        "                sd_target[key] += scores[i] * dict[key]\n",
        "            sd_target[key] /= sum(scores)\n",
        "\n",
        "        self.global_agent.online_net.load_state_dict(sd_online)\n",
        "        self.global_agent.target_net.load_state_dict(sd_target)\n",
        "\n",
        "\n",
        "    def set_local_networks(self):\n",
        "        for agent in self.agents:\n",
        "            agent.online_net.load_state_dict(\n",
        "                self.global_agent.online_net.state_dict())\n",
        "            agent.target_net.load_state_dict(\n",
        "                self.global_agent.target_net.state_dict())"
      ],
      "metadata": {
        "id": "RvupvVEmfga_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Federator(create_mario_env, 200, load=True)\n",
        "agent.train(5)"
      ],
      "metadata": {
        "id": "13Si-HfXFpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r ./Mario/ /content/drive/Shareddrives/Sam/"
      ],
      "metadata": {
        "id": "xfbDQKYyH9qP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}