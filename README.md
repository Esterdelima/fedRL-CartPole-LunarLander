
# ğŸ§  Aprendizado por ReforÃ§o Federado com Deep Q-Networks

Este projeto investiga o uso de **Aprendizado Federado (FL)** aplicado ao **Aprendizado por ReforÃ§o Profundo (Deep Reinforcement Learning - DRL)**. A proposta Ã© avaliar se mÃºltiplos agentes podem aprender de forma descentralizada, compartilhando apenas os parÃ¢metros dos modelos, sem transmitir experiÃªncias ou dados brutos.

---

## ğŸ¯ Objetivo

Avaliar se o uso de estratÃ©gias de agregaÃ§Ã£o como **FedAvg** e **FedProx**:

- Aumenta a eficiÃªncia amostral  
- Melhora a performance global  
- Promove generalizaÃ§Ã£o  
- Permite reaproveitamento da polÃ­tica aprendida  

A comparaÃ§Ã£o Ã© feita com agentes isolados (single-agent), em cenÃ¡rios homogÃªneos (IID) e heterogÃªneos (nÃ£o-IID).

---

## ğŸ“¦ Estrutura do Projeto

```bash
â”œâ”€â”€ federated/                 # MÃ³dulos federados (Agente, Federator, Buffer, Q-Network)
â”œâ”€â”€ single_agent/              # ImplementaÃ§Ã£o para agente isolado (nÃ£o federado)
â”œâ”€â”€ results/                   # Recompensas, modelos, grÃ¡ficos e vÃ­deos
â”œâ”€â”€ main-cart.py               # Treinamento federado em CartPole-v1
â”œâ”€â”€ main-lun.py                # Treinamento federado em LunarLander-v2
â”œâ”€â”€ main_noniid_agents.py      # Treinamento federado com ambientes diferentes (nÃ£o-IID)
â”œâ”€â”€ single-agent-cart.py       # Treinamento single-agent no CartPole
â”œâ”€â”€ single-agent-lun.py        # Treinamento single-agent no LunarLander
â”œâ”€â”€ visualize_agent_gif.py     # Script para renderizaÃ§Ã£o e visualizaÃ§Ã£o dos agentes
â”œâ”€â”€ requirements.txt           # DependÃªncias do projeto
```

---

## ğŸŒ Ambientes Suportados

- ğŸŸ¢ `CartPole-v1` â€” ambiente simples de equilÃ­brio de haste  
- ğŸŸ¢ `LunarLander-v2` â€” pouso 2D com mÃºltiplos sensores e aÃ§Ãµes  
- ğŸ”„ `SuperMarioBros` â€” planejado como trabalho futuro  

---

## ğŸ§  Algoritmos de RL

- âœ… **Deep Q-Network (DQN)**  
- âœ… **Double DQN (DDQN)**  
- ğŸ”„ *Outros algoritmos como PPO e A3C podem ser integrados futuramente*

---

## âš™ï¸ EstratÃ©gias Federadas

- `FedAvg` â€” mÃ©dia ponderada dos modelos dos agentes locais  
- `FedProx` â€” variante com regularizaÃ§Ã£o para lidar com ambientes heterogÃªneos  
- `Single-Agent` â€” baseline com agente isolado (centralizado)  

---

## ğŸ§ª Como Rodar os Experimentos

> Recomendado: Python 3.10+ com ambiente `conda` ou `venv`.

### 1. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

Ou via Conda:

```bash
conda create -n fedrl python=3.10
conda activate fedrl
pip install -r requirements.txt
```

---

### 2. Treinamento Federado

#### CartPole

```bash
python main-cart.py
```

#### LunarLander

```bash
python main-lun.py
```

#### Ambientes Diferentes (nÃ£o-IID)

```bash
python main_noniid_agents.py
```

---

### 3. Treinamento Isolado (Single-Agent)

#### CartPole

```bash
python single-agent-cart.py
```

#### LunarLander

```bash
python single-agent-lun.py
```

---

### 4. VisualizaÃ§Ã£o do Comportamento (GIF/Render)

Use o script:

```bash
python visualize_agent_gif.py
```

> O arquivo `visualize_agent_gif.py` gera um `.gif` do comportamento do agente (global ou local) apÃ³s o treinamento.

---

## ğŸ“ˆ Resultados

Os resultados sÃ£o salvos automaticamente em `./results/`:

- `rewards.npy`: recompensas por episÃ³dio  
- `comparison_plot.png`: grÃ¡fico comparando estratÃ©gias  
- `render_global_*.gif`: visualizaÃ§Ã£o do comportamento do agente global  

---

## ğŸ“Œ Trabalhos Futuros

- Adicionar suporte ao ambiente `SuperMarioBros`  
- Integrar algoritmos avanÃ§ados como PPO e A3C  
- Avaliar reuso de polÃ­ticas globais via Fine-Tuning  
- Analisar robustez em cenÃ¡rios com agentes adversariais ou com falhas  

---

## ğŸ“š ReferÃªncias AcadÃªmicas

Este projeto se baseia em estudos recentes da literatura de FRL, como:

- Zhuo et al. â€” *Federated Deep Reinforcement Learning*  
- Fan et al. â€” *Fault-Tolerant Federated RL*  
- Jin et al. â€” *FRL with Environment Heterogeneity*  
- Qi et al. â€” *Survey on FRL*  
- Neto et al. â€” *FRL for IoT*  

---

## ğŸ‘©â€ğŸ’» Autoria

Este projeto foi desenvolvido como parte do trabalho final da disciplina **Aprendizado por ReforÃ§o**, com foco em reprodutibilidade, leveza computacional e extensÃ£o de estudos do estado da arte.
